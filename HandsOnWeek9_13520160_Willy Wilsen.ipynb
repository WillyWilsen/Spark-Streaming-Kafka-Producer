{"cells":[{"cell_type":"markdown","metadata":{"id":"l6oyk4Z7kaJz"},"source":["# HandsOn Week 9\n","Welcome to HandsOn Week 9. In this HandsOn, you will try to play with spark streaming where the data is from a Kafka producer."]},{"cell_type":"markdown","metadata":{"id":"6GS72DtXkaKF"},"source":["## Setting up\n","Since there SparkStreaming from Kafka is not supported in Spark version 3.1.1, There are some things that you need to setup or install:\n","1. You need to download apache spark version 2.7.4 with hadoop 2.7 [https://spark.apache.org/downloads.html]\n","2. Unzip the tgz file\n","3. Open bashrc file `nano ~/.bashrc`. Then, find those variables and set the values to\n","    * SPARK_HOME=~/Downloads/spark-2.4.7-bin-hadoop2.7\n","    * PYSPARK_PYTHON=python3.7\n","4. Activate `source ~/.bashrc`"]},{"cell_type":"markdown","metadata":{"id":"WN3x1icpkaKH"},"source":["## Milestone 1\n","You need to install kafka-python by ```pip install kafka-python```.<br><br>\n","In this milestone, you only need to run ```producer_variance.py``` and ```consumer_variance.py``` (these two code files are already provided inside the zip file)."]},{"cell_type":"markdown","metadata":{"id":"grR-M5YAkaKJ"},"source":["Screenshot your ```consumer_variance.py``` output, and put in this cell below. \n","\n","<img title=\"Milestone 1\" align=\"left\" src=\"milestone1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"]},{"cell_type":"markdown","metadata":{"id":"QmQD9WOKkaKL"},"source":["## Milestone 2\n","After making sure that the message is published by ```producer_variance.py``` and successfully consumed by ```consumer_variance.py``` in the topic of ```variance``` in the Milestone 1 above, then, you are ready for Milestone 2.<br>\n","\n","In Milestone 2, you need to implement ```calculate_variance``` function with the formula --> $variance = \\frac{\\sum_{i=1}^{N}x_i^2}{N}-(\\frac{\\sum_{i=1}^{N}x_i}{N})^2$. This function will be used to calculate variance for each window operation to the streaming data, and the variance is \"**accumulative/global**\" value up to current stream data. For example, in the first window, we have data ```1,2,3```, then the variance is the variance of ```1,2,3```. Let's say we have streaming data of ```4,5,6``` in the second window, thus the variance in this second window is the variance of ```1,2,3,4,5,6```, and so on for the following windows.<br>\n","\n","The ```calculate_variance``` function will return a DStream (RDD) with a format of ```('sum_x_square:', sum_x_square_value, 'sum_x:', sum_x_value, 'n:', n_value, 'var:', variance_value)``` where ```sum_x_square_value```$=\\sum_{i=1}^{N}x_i^2$, ```sum_x_value```$=\\sum_{i=1}^{N}x_i$ and ```n```$=N$. Note that $x_i=$ i-th of individual stream data, and $N=$ the number of individual stream data -count- up to i-th data.\n","\n","**Important:** In order to stream from Kafka producer to Spark Streaming, you need to download [spark-streaming-kafka-0-8-assembly_2.11-2.4.7.jar](https://mvnrepository.com/artifact/org.apache.spark/spark-streaming-kafka-0-8-assembly_2.11/2.4.7) from maven repository (adjust with your Spark version), and put this jar file to ```your_spark_folder/jars```. For VM provided by the class, ```spark_folder``` is in ```/home/bigdata/spark-2.4.5-bin-hadoop2.7```."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5PSRy-DNkaKR"},"outputs":[],"source":["from pyspark.streaming import StreamingContext\n","from pyspark.streaming.kafka import KafkaUtils\n","\n","KAFKA_TOPIC = \"variance\"\n","BOOTSTRAP_SERVER = \"localhost:9092\"\n","\n","ssc = StreamingContext(sc,1) #stream each one second\n","ssc.checkpoint(\"./checkpoint\")\n","lines = KafkaUtils.createDirectStream(ssc, [KAFKA_TOPIC],\n","                                      {\"metadata.broker.list\": BOOTSTRAP_SERVER})\n","\n","def calculate_variance(lines, window_length = 2, sliding_interval = 2):\n","    \"\"\"\n","    Function to calculate \"accumulated/global variance\" in each window operation\n","    Params:\n","        lines: Spark DStream defined above (in this jupyter cell)\n","        window_length: length of window in windowing operation\n","        sliding_interval: sliding interval for the window operation\n","    Return:\n","        result: DStream (RDD) of variance result with \n","                format --> ('sum_x_square:', sum_x_square_value, 'sum_x:', sum_x_value, 'n:', n_value, 'var:', variance_value)\n","                Example:   ('sum_x_square:', 182.0, 'sum_x:', 42.0, 'n:', 12.0, 'var:', 2.916666666666666)\n","    \"\"\"\n","    # Realize this function here. Note that you are not allowed to modify any code other than this function.\n","    def compute_variance(rdd):\n","        values = rdd.collect()\n","        if len(values) > 0:\n","            n = len(values)\n","            sum_x = sum(values)\n","            sum_x_square = sum(x**2 for x in values)\n","            variance = (sum_x_square / n) - (sum_x / n)**2\n","            result = [('sum_x_square:', sum_x_square, 'sum_x:', sum_x, 'n:', n, 'var:', variance)]\n","        else:\n","            result = []\n","        return rdd.context.parallelize(result)  # Convert the list to an RDD\n","\n","    numbers = lines.flatMap(lambda line: line[1].split(\" \")).map(lambda number: float(number))\n","    windowed_numbers = numbers.window(window_length, sliding_interval)\n","    result = windowed_numbers.transform(compute_variance)\n","    \n","    return result\n","\n","# run the function\n","result = calculate_variance(lines, window_length=2, sliding_interval=2)\n","# Print\n","result.pprint()\n","ssc.start()\n","ssc.awaitTermination()"]},{"cell_type":"markdown","metadata":{},"source":["<img title=\"Milestone 2\" align=\"left\" src=\"milestone2.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"]},{"cell_type":"markdown","metadata":{"id":"42q5PgxKkaKZ"},"source":["## Submission\n","Archive this ipynb file and the screenshot image needed in the Milestone 1 into zip file with a format of: ```HandsOnWeek11_NIM_FullName.zip```, and submit to the submission form.\n","\n","**Note**: make sure in the Milestone 2, the cell has its output, but not too many streams (you can save this ipynb file approximatelly in the range of 4-20 window operations)\n","\n","Enjoy..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AXbpl0nUkaKc"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}
